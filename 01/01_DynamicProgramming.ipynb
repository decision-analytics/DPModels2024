{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"img/bigsem.png\" width=\"40%\" align=\"right\">\n",
    "<img src=\"img/logo_wiwi.png\" width=\"20%\" align=\"left\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br><br>\n",
    "\n",
    "# Dynamic Programming Models in Combinatorial Optimization\n",
    "**Winter Term 2022/23**\n",
    "\n",
    "\n",
    "# 1. Introduction. DP Models \n",
    "\n",
    "<img src=\"img/decision_analytics_logo.png\" width=\"17%\" align=\"right\">\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**J-Prof. Dr. Michael RÃ¶mer |  Decision Analytics Group**\n",
    "                                                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit\n",
    "from typing import NamedTuple, Callable\n",
    "import keyboard\n",
    "from IPython.display import SVG, display, clear_output\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "- Course Goals and Organization\n",
    "- Combinatorial Optimization Problems and Greedy Algorithms:\n",
    "  - Illustrated using 0/1 Knapsack and TSP\n",
    "- From Greedy to Dynamic Programming (Models)\n",
    "- Exact and Approximate Solution using Exact DP and Beam Search\n",
    "- (Online) Value Function Approximation using Rollout\n",
    "- Wrapping up and Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course Goal\n",
    "\n",
    "#### In this course, you learn\n",
    "- how to model Combinatorial Cptimization problems as Dynamic Programming Models\n",
    "- about various ways in which these models can be used to solve Combinatorial Optimization Problems\n",
    "\n",
    "#### Ideally, after the course\n",
    "- you are able to employ these techniques in your own research\n",
    "- and have a paper draft using the techniques from this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Course Outline\n",
    "\n",
    "\n",
    "#### Part 1 (starting now): Integrated Lecture / Exercise\n",
    "- lecture-style content presentation from Jupyter-based slides\n",
    "- Python implementation and hands-on exercises\n",
    "\n",
    "\n",
    "\n",
    "#### Part 2: Course Projects\n",
    "- a small group project related to the course topics\n",
    "- ideally related to your PhD \n",
    "- presentation of the project **(Mar)** and project report / paper draft\n",
    "- great opportunity for starting collaboration, we already had publications emanating from these projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grading / Coursework \n",
    "\n",
    "#### If you do not need a grade / proof of participation, feel free to\n",
    "- participate in the whole course or\n",
    "- participate in the parts of the course that seem helpful to you\n",
    "\n",
    "\n",
    "**BiGSEM-students** course can use this course as **elective course**:\n",
    "- paper and project presentations as \"ungraded academic achievement\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##  Part I: Schedule and Preliminary Contents\n",
    "\n",
    "\n",
    "We will have **four blocks** of **2 x 90 h**, **Thursdays at 12:30 h** in **V10-122**\n",
    "\n",
    "\n",
    "#### Block 1: Heuristics Based on DP Models\n",
    "- from greedy to dynamic programming\n",
    "- dynamic programming models\n",
    "- exactly solving DP models using DP by reaching\n",
    "- heuristically solving DP models by beam search\n",
    "- approximation in value space using rollout\n",
    "- relation to reinforcement learning\n",
    "\n",
    "\n",
    "#### Block 2: Decision Diagrams (and DIDP)?\n",
    "- exact decision diagrams from DP models\n",
    "- top-down compilation\n",
    "- reducing DDs\n",
    "- restricted and relaxed decision diagrams\n",
    "- DD-based branch and bound\n",
    "\n",
    "#### Block 3:  DP Model-based MIPs  (I)\n",
    "- solving DP Models by embedding their state-transition graphs in MIPs\n",
    "- employing DD reduction for DP models in MIPs\n",
    "- the power of (multi-unit) flows in state-transition graphs (state-expanded networks)\n",
    "\n",
    "\n",
    "#### Block 4: DP Model-based MIPs   (II)\n",
    "- state-expanded networks for representing complex constraints such as working rules\n",
    "- single networks versus multiple networks\n",
    "- filtering state-expanded networks using machine learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Course Material\n",
    "\n",
    "- course material can be found in Moodle\n",
    "- it is in form of Juypter notebooks\n",
    "- attention: it also involves instance files, so\n",
    "  - make sure you know how to read in files in Colab\n",
    "  - or use a local installation of Python\n",
    "  - or use the Binder link provided in the Lernraum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Combinatorial optimization problems\n",
    "\n",
    "- combinatorial optimization (CO) problems are discrete optimization problems, that is, optimization problems in which the set of feasible solutions is discrete\n",
    "- well-known examples for CO problems are\n",
    "  - the the travelling salesperson problem (TSP)\n",
    "  - the 0/1 knapsack problem\n",
    "  - the set covering problem\n",
    "- many CO problems are NP-hard, that is, there no known algorithm that can solve them in polynomial time\n",
    "- in this course, we will learn \n",
    "  - how (certain) CO problems can be cast as dynamic programming models\n",
    "  - and how such models can be useful for solving CO problems approximately or exactly\n",
    "  - in very different ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greedy Approaches to Combinatorial Optimization Problems\n",
    "- given that exactly solving (large) CO problems is often not (practically) tractable, one often resorts to heuristic approaches that aim at\n",
    "  - finding high-quality solutions\n",
    "  - in an acceptable amount of time\n",
    "- one class of heuristic solutions that are usually *very* fast (but not alway yield high-quality solutions) are so-called **greedy algorithms**\n",
    "- greedy algorithms solve a CO problem by construction a solution step by step\n",
    "- in every step, a decision is made according to a **greedy criterion**:\n",
    "  - in general, in case of different feasible options, the one that is (locally) optimal with respect to the greedy criterion is taken\n",
    "   - example: the nearest neighbor algorithm for solving the TSP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: the 0/1 knapsack problem\n",
    "\n",
    "Given \n",
    "- a knapsack with a capacity $W$ \n",
    "- and a set of items, each with a weight $w_i$ and a value $p_i$\n",
    "- determine the the subset of the items to put in the knapsack such that\n",
    "  - the total value of the items in the knapsack is maximal and\n",
    "  - the total weight of the items in the knapsack does not exceed $W$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Example:**\n",
    "\n",
    "<img src=\"./img/greedy/07.png\" width=\"20%\" align=\"right\">\n",
    "\n",
    "Assume you are a thief and you are about to steal the three items depicted below from an appartment. However, your backpack can only fit 35 lbs. Which items should you take?\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./img/greedy/08.png\" width=\"40%\" align=\"left \">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Greedy Approach for the Knapsack Problem\n",
    "\n",
    "- start with some item: If it (still) fits in the backpack, put it in the backpack\n",
    "- repeat for the remaining items\n",
    "\n",
    "..you never take out an item once it has been packed in the knapsack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_knapsack(values, weights, capacity):\n",
    "    solution = [] # solution array\n",
    "    obj_val = 0 # accumulated objective\n",
    "    total_weight = 0 # accumulated weight\n",
    "    \n",
    "    for i, weight in enumerate(weights): \n",
    "        if total_weight + weight <= capacity: ## if the item still fits..\n",
    "            solution.append(i) ## add it and \n",
    "            total_weight+= weight # update the accumulated weight\n",
    "            obj_val += values[i] # as well as the optimal objective value\n",
    "    return obj_val, solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..let us try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, [0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = [3000,2000,1500]\n",
    "weights = [30,20,15]\n",
    "capacity = 35\n",
    "\n",
    "greedy_knapsack(values, weights, capacity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let us try larger instances\n",
    "\n",
    "..there are many instance sets for the 0/1 KP\n",
    "- as an example, there are some instances from D. Pisinger, see the instances folder in the repository associated with this notebook\n",
    "- on the following website, you will find optimal objective function values:\n",
    "\n",
    "http://artemisa.unicauca.edu.co/~johnyortega/instances_01_KP/\n",
    "\n",
    ".. you find some instances in the GitHub repository in which this notebook resides\n",
    "- if you download the zip with this notebook (or clone the repository), you will have them in the folder `problems/knapsack/instances`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Format of the knapsack instances\n",
    "\n",
    "the instance files have the following format:\n",
    "\n",
    "- first row: `number_of_items` `capacity`\n",
    "- every further row contains informatin for each item: `value` `weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our toy instance would look like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "3 35\n",
    "3000 30\n",
    "2000 20\n",
    "1500 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading in the instances\n",
    "\n",
    "The following function reads an instance file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def read_knapsack_instance(filename):\n",
    "    weights=[]\n",
    "    values=[]\n",
    "    with open(filename) as f: # open the file\n",
    "        line = f.readline().split()  # split first row\n",
    "        number_of_items = int(line[0]) # read number of items\n",
    "        capacity = int(line[1]) # read capacity\n",
    "        for i in range(number_of_items): # read rows for the items\n",
    "            line = f.readline().split() # split row\n",
    "            values.append(int(line[0])) # read value\n",
    "            weights.append(int(line[1])) # read weight\n",
    "    return np.array(values), np.array(weights), capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case you use colab:\n",
    "\n",
    "#!pip install python-tsp\n",
    "#!npx degit decision-analytics/DPModels2024/problems -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... let us try with a 5000-item instance and solve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33727\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filename = \"./../problems/knapsack/instances/knapPI_1_5000_1000_1\" # optimal value: 276457 \n",
    "values, weights, capacity  = read_knapsack_instance(filename)\n",
    "\n",
    "obj_value, _ = greedy_knapsack(values, weights, capacity)\n",
    "print(obj_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Improving the greedy approach by sorting items\n",
    "\n",
    "- one way to improve the performance of this greedy algorithm for the knapsack problem is to sort items\n",
    "- which sorting criteria do you consider promising?\n",
    "- sort the items accordingly and try applying the greedy algorithm to the sorted items\n",
    "\n",
    "#### Hint:\n",
    "In numpy, there is the function `argsort` which does not return the sorted values of an array, but an array of the sorted sorted indexes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#filename = \"./../problems/knapsack/instances/knapPI_1_5000_1000_1\" # optimal value: 276457\n",
    "\n",
    "\n",
    "#obj_value, _ = greedy_knapsack(sorted_values, sorted_weights, capacity)\n",
    "#obj_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Travelling Salesperson Problem\n",
    "\n",
    "<img src=\"https://pup-assets.imgix.net/onix/images/9780691163529.jpg\" width=\"20%\" align=\"right\">\n",
    "\n",
    "\n",
    "**Informal problem statement:** Given a set of cities and the distances between the cities, find a minimum-cost round-trip that visits each city exactly once.\n",
    "\n",
    "**More formally:** Given a complete graph and distances between each pair of nodes in the graph, find a cost-minimal hamiltonian cycle in the graph\n",
    "\n",
    "\n",
    "- one of the best-known combinatorial opimization problem \n",
    "- **A nice book on the TSP:**  [In Pursuit of the Traveling Salesman](https://press.princeton.edu/books/paperback/9780691163529/in-pursuit-of-the-traveling-salesman)\n",
    " - die story of the TSP presented by one of its protagonists (William Cook)\n",
    "- TSP website: https://www.math.uwaterloo.ca/tsp/index.html\n",
    "- there are a lot of instances\n",
    "    - in particular, there is a full library of instances, the so-called [TSPLib](http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/)\n",
    "    - some of them are part of the git repository for the course material\n",
    "    - [here](http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html) you find optimal objective values for many instances\n",
    "\n",
    "**..and there is even a Python library dedicated to solving the TSP: [`python-tsp`](https://github.com/fillipe-gsm/python-tsp)** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nearest Neighbor: A greedy algorithm for the TSP\n",
    "\n",
    "- we assume that the cities are indexed from 0 to $N-1$\n",
    "\n",
    "Goal: create a list forming a permutation of the city indexes representing a tour with a small total distance \n",
    "- start with some node and add it to the list\n",
    "- find a node that is not yet in the list and that is nearest to the most recently added node and add it to the list\n",
    "- repeat until the list has length $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A helper function that computes the nearest neighbor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_nearest_neighbor(distance_matrix, permutation):\n",
    "    \n",
    "    # node the last node in the permutation\n",
    "    node = permutation[len(permutation)-1]\n",
    "    \n",
    "    smallest_distance = 9999999999 ## some large value\n",
    "    nearest_neighbor = 0\n",
    "    \n",
    "    #number of nodes = dimension of the distance matrix\n",
    "    for neighbor in range(len(distance_matrix)):\n",
    "        \n",
    "        if neighbor in permutation: continue # skip if already visited\n",
    "        \n",
    "        #update the nearest neighbor if needed\n",
    "        if distance_matrix[node][neighbor] < smallest_distance: \n",
    "            nearest_neighbor = neighbor\n",
    "            smallest_distance = distance_matrix[node][neighbor]            \n",
    "       \n",
    "    return nearest_neighbor, smallest_distance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The full algorithm in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def tsp_nearest_neighbor(distance_matrix, permutation):\n",
    "    \n",
    "    total_distance = 0\n",
    "    \n",
    "    #as long as the list is not \"full\"\n",
    "    while len(permutation) < len(distance_matrix):\n",
    "        \n",
    "        node, distance = get_nearest_neighbor(distance_matrix, permutation)\n",
    "        \n",
    "        permutation.append(node)\n",
    "        total_distance += distance\n",
    "        \n",
    "    total_distance += distance_matrix[permutation[len(permutation)-1], permutation[0]] # final\n",
    "    return permutation, total_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let us try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 2, 3, 1], np.int64(17))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_matrix_tiny = np.array([\n",
    "    [0,  5, 4, 10],\n",
    "    [5,  0, 8,  5],\n",
    "    [4,  8, 0,  3],\n",
    "    [10, 5, 3,  0]\n",
    "])\n",
    "\n",
    "tsp_nearest_neighbor(distance_matrix_tiny, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Python library `python-tsp`\n",
    "\n",
    "see: https://github.com/fillipe-gsm/python-tsp\n",
    "\n",
    "### offers:\n",
    "- functions to read TSP instances in the tsplib-format\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(30774)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from python_tsp.distances import tsplib_distance_matrix\n",
    "\n",
    "#tsplib_file = \"./../problems/tsp/instances/a280.tsp\" # optimal solution 2579 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "tsplib_file = \"./../problems/tsp/instances/brazil58.tsp\" # optimal solution 25395 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "#tsplib_file = \"./../problems/tsp/instances/berlin52.tsp\" # optimal solution  7542 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "\n",
    "distance_matrix = tsplib_distance_matrix(tsplib_file)\n",
    "\n",
    "permutation, distance = tsp_nearest_neighbor(distance_matrix, [0])\n",
    "distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- and heuristic as well as exact TSP algorithms\n",
    "  - e.g. local search, simulated annealing and dynamic programming (exact: careful, may take very long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from python_tsp.heuristics import solve_tsp_local_search, solve_tsp_simulated_annealing\n",
    "\n",
    "#permutation, distance = solve_tsp_local_search(distance_matrix)\n",
    "\n",
    "#permutation, distance = solve_tsp_simulated_annealing(distance_matrix)\n",
    "#distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Improving nearest neighbor: multi-start\n",
    "\n",
    "- one way to improve a greedy heuristic such as nearest neighbor that relies parameters (here: start city) is to call the greedy algorithm multiple times with different parameters\n",
    "- in general, many greedy algorithms are so fast that calling them multiple times is a perfectly feasible approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Task: Write a function that calls the nearest-neighbor algorithm for every possible start node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def tsp_multi_start_nearest_neighbor(distance_matrix):\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#permutation, distance = tsp_multi_start_nearest_neighbor(distance_matrix)\n",
    "#distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speeding up with numba\n",
    "\n",
    "**`numba`**\n",
    "\n",
    "-  among other things, numba allows to **just-in-time** compile Python code\n",
    "- this make Python code much faster\n",
    "- but it only applies to a certain subset of Python\n",
    "- see https://numba.pydata.org/ for more information\n",
    "\n",
    "- most simple approach to apply numba use the *decorators* `@njit` above the function to just-in-time compile\n",
    "- let us try this out with the function `tsp_multi_start_nearest_neighbor` and re-do the timing \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Greedy to Dynamic Programming (Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling a discrete multi-stage transition system\n",
    "\n",
    "<img src=\"./img/deterministic_multistage_problem.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "\n",
    "- $k$ the current step / stage (e.g. the number of cities visited so far), out of $N$ stages.\n",
    "- $x_k$ the current state needed to calculate the next step and the cost\n",
    "    - e.g. the cities visited visited so far and the current city\n",
    "    - the start state is defined as $x_0$\n",
    "- $u_k$ a decision from the set $U_k(x_k)$ of feasible decisions when being in stage $k$ and in state $x_k$ \n",
    "  - e.g. a city that was not visited so far  \n",
    "- $g(x_k, u_k)$ the cost of choosing decision $u_k$ when being in state $x_k$\n",
    "  - e.g. the distance to the next city\n",
    "- $f(x_k, u_k)$ a transition function that computes $x_{k+1}$ from $x_k$ und the decision $u_k$ \n",
    "  - e.g. an augmentation of the cities visited so far and an update of the current city\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A dynamic programming model\n",
    "\n",
    "A model for such a discrete system as defined on the previous slide along with the optimization problem:\n",
    "\n",
    "$$\\min_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n",
    "\n",
    "\n",
    "..will be referred to as a **dynamic programming model** in this remainder of this course, and we will refer to this generic problem as $DP$ in what follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Observe:\n",
    "- here, we assume a minimization problem, but it is straightforward to obtain a corresponding maximization problem\n",
    "- we also assume the cost are \"stage-wise\"-additive (but: they can be state-dependent!)\n",
    "- we assume there are no terminal costs $g(x_N)$ (would be straightforward to include)\n",
    "- there can be far more general DP models, but for now we stick to classes that can be represented as displayed above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: A DP model for the Knapsack Problem\n",
    "\n",
    "Given:\n",
    "- a knapsack instance with $N$ items with weights $w_k$ and profits $p_k$ (zero-indexed) and capacity $W$ \n",
    "\n",
    "- state $x_k$: accumulated weight after adding the first $k-1$ items, $x_0 = 0$\n",
    "- decision $u_k \\in \\{0, 1\\}$ (0: do not add item $k$ to the knapsack; 1: add item $k$)\n",
    "- $U_k(x_k) = \\begin{cases} \n",
    "                \\{0,1\\} \\quad \\mathrm{if} \\quad x_k + w_k \\leq W \\\\\n",
    "                \\{0 \\} \\quad \\mathrm{else}\n",
    "\\end{cases}$\n",
    "- $f(x_k, u_k) = x_k + w_k u_k $\n",
    "- $g(x_k, u_k) = p_k u_k$\n",
    "\n",
    "We have a maximization objective:\n",
    "\n",
    "$$\\max_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: A DP model for the TSP\n",
    "\n",
    "Given:\n",
    "- a TSP instance with a $N$ cities and distances $d_{i,j}$ between cities $i,j$\n",
    "  - let us denote with $\\mathcal{N} = \\{1, \\ldots N \\}$ the set of cities \n",
    "  \n",
    "\n",
    "\n",
    "- state $x_k$: sequence / ordered set of cities visited so far, $x_0 = i^0$ where $i^0$ is the first city\n",
    "  - let us define $l(x_k)$ as the last element in the ordered set, that is, the \"current\" city\n",
    "\n",
    "- decision $u_k \\in \\mathcal{N}$ city to visit next \n",
    "- $U_k(x_k) = \\mathcal{N} \\setminus x_k$\n",
    "- $f(x_k, u_k) = x_k + u_k$  (here, with $+$ we mean to append $u_k$ to the sequence / ordered set $x_k$\n",
    "- $g(x_k, u_k) = \\begin{cases} \n",
    "                d_{l(x_k), u_k} \\quad \\mathrm{if} \\quad k < N-1 \\\\\n",
    "               d_{l(x_k), u_k} +  d_{u_k, i^0} \\quad  \\mathrm{if} \\quad k = N-1\n",
    "\\end{cases}$\n",
    "\n",
    "We have a minimization objective:\n",
    "\n",
    "$$\\min_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing a DP model in Python\n",
    "\n",
    "Let us devise a Python framework for implementing a dynamic programming model.\n",
    "\n",
    "To implement a DP model for a CO problem in this framework, we need to implement the following functions:\n",
    "- a function returning the feasible decisions $U_k$ given a state $x_k$ in stage $k$ (and the data from the instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def feasible_decisions(instance, k, state):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the transition function $f(x_k, u_k)$ returning the state $x_{k+1}$ resulting from taking decision $u_k$ when being in state $x_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def transition_function(instance, k, state, decision):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(instance, k, state, decision):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Representing a DP model in Python\n",
    "\n",
    "- we can collect these three functions in a `NamedTuple` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP(NamedTuple):\n",
    "    feasible_decisions : Callable\n",
    "    transition_function : Callable\n",
    "    cost_function : Callable\n",
    "    direction : str # 'max' or 'min'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### States\n",
    "- we are free to define our state representation\n",
    "- for later purposes, it will be useful if the state variable is immutable, therefore tuples or namedtuple are useful data structures for states\n",
    "\n",
    "\n",
    "#### Decisions\n",
    "- in most cases in this part, we will assume that decisions are integers, but note that this is not required as long as the transition function works\n",
    "- however, for now, we assume that decisions only induce a change between stages -- we will relax that requirement later in the course\n",
    "\n",
    "\n",
    "#### Instance data\n",
    "- all functions named above take an instance as parameter. Instance data does not have to take a certain form, it just needs to \"match\" the (problem-specific) functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic helper functions to deal with maximization and minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit \n",
    "def better(value1, value2, direction):\n",
    "    if direction == \"min\":\n",
    "        return value1 < value2\n",
    "    else:\n",
    "        return value1 > value2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "better(3,4,\"min\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def best(collection, direction):\n",
    "    if direction == \"min\":\n",
    "        return min(collection)\n",
    "    else:\n",
    "        return max(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "example_values = [2,8,7,1,5]\n",
    "\n",
    "print(best(example_values,\"min\"))\n",
    "print(best(example_values,\"max\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from heapq import nsmallest\n",
    "from heapq import nlargest\n",
    "\n",
    "@njit \n",
    "def nbest(n,collection, direction):\n",
    "    if direction == \"min\":\n",
    "        return nsmallest(n,collection)\n",
    "    else:\n",
    "        return nlargest(n,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "[8, 7]\n"
     ]
    }
   ],
   "source": [
    "print(nbest(2,example_values,\"min\"))\n",
    "print(nbest(2,example_values,\"max\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: A DP model for the Knapsack Problem\n",
    "\n",
    "Given:\n",
    "- a knapsack instance with $N$ items with weights $w_k$ and profits $p_k$ (zero-indexed) and capacity $W$ \n",
    "\n",
    "- state $x_k$: accumulated weight after adding the first $k-1$ items, $x_0 = 0$\n",
    "- decision $u_k \\in \\{0, 1\\}$ (0: do not add item $k$ to the knapsack; 1: add item $k$)\n",
    "- $U_k(x_k) = \\begin{cases} \n",
    "                \\{0,1\\} \\quad \\mathrm{if} \\quad x_k + w_k \\leq W \\\\\n",
    "                \\{0 \\} \\quad \\mathrm{else}\n",
    "\\end{cases}$\n",
    "- $f(x_k, u_k) = x_k + w_k u_k $\n",
    "- $g(x_k, u_k) = p_k u_k$\n",
    "\n",
    "We have a maximization-objective:\n",
    "\n",
    "$$\\max_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Knapsack DP Model in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "class  KPInstance(NamedTuple):\n",
    "    values:np.array\n",
    "    weights:np.array\n",
    "    capacity:int\n",
    "    N:int   \n",
    "\n",
    "@njit\n",
    "def feasible_decisions_kp(instance, k, acc_weight):    \n",
    "    if acc_weight + instance.weights[k] <= instance.capacity: return [0,1]\n",
    "    else: return [0]\n",
    "\n",
    "@njit\n",
    "def transition_function_kp(instance, k, acc_weight, put):\n",
    "    return acc_weight + put*instance.weights[k]\n",
    "\n",
    "@njit\n",
    "def cost_function_kp(instance, k, acc_weight, put):\n",
    "      return put*instance.values[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Putting all together, and stating that we have a maximization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dp_kp = DP(feasible_decisions_kp, transition_function_kp,  cost_function_kp, \"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An instance reader function for the Knapsack Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_kp_instance(filename, sorted=True):\n",
    "    weights=[]\n",
    "    values=[]\n",
    "    with open(filename) as f: # open the file\n",
    "        line = f.readline().split()  # split first row\n",
    "        number_of_items = int(line[0]) # read number of items\n",
    "        capacity = int(line[1]) # read capacity\n",
    "        for i in range(number_of_items): # read rows for the items\n",
    "            line = f.readline().split() # split row\n",
    "            values.append(int(line[0])) # read value\n",
    "            weights.append(int(line[1])) # read weight\n",
    "            \n",
    "    values = np.array(values)\n",
    "    weights = np.array(weights)    \n",
    "    \n",
    "    \n",
    "    if sorted:\n",
    "        sorted_indexes = np.argsort(-1* values/weights)\n",
    "    values = values[sorted_indexes]\n",
    "    weights = weights[sorted_indexes]\n",
    "     \n",
    "        \n",
    "    return KPInstance(values, weights, capacity, number_of_items)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"./../problems/knapsack/instances/knapPI_1_5000_1000_1\"\n",
    "filename = \"./../problems/knapsack/instances/knapPI_1_100_1000_1\"\n",
    "kp_instance = read_kp_instance(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: A DP model for the TSP\n",
    "\n",
    "Given:\n",
    "- a TSP instance with a $N$ cities and distances $d_{i,j}$ between cities $i,j$\n",
    "  - let us denote with $\\mathcal{N} = \\{1, \\ldots N \\}$ the set of cities \n",
    "  \n",
    "\n",
    "\n",
    "- state $x_k$: sequence / ordered set of cities visited so far, $x_0 = i^0$ where $i^0$ is the first city\n",
    "  - let us define $l(x_k)$ as the last element in the ordered set, that is, the \"current\" city\n",
    "\n",
    "- decision $u_k \\in \\mathcal{N}$ city to visit next \n",
    "- $U_k(x_k) = \\mathcal{N} \\setminus x_k$\n",
    "- $f(x_k, u_k) = x_k + u_k$  (here, with $+$ we mean to append $u_k$ to the sequence / ordered set $x_k$\n",
    "- $g(x_k, u_k) = \\begin{cases} \n",
    "                d_{l(x_k), u_k} \\quad \\mathrm{if} \\quad k < N-1 \\\\\n",
    "               d_{l(x_k), u_k} +  d_{u_k, i^0} \\quad  \\mathrm{if} \\quad k = N-1\n",
    "\\end{cases}$\n",
    "\n",
    "We have a minimization objective:\n",
    "\n",
    "$$\\min_{u_0,..,u_k,..u_{N-1}} \\sum_{k=0}^{N-1} g_k(x_k,u_k)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: DP model for the TSP in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class TSPInstance(NamedTuple):\n",
    "    distance_matrix : np.array\n",
    "    N : int\n",
    "\n",
    "@njit\n",
    "def feasible_decisions_tsp(instance, k, sequence):\n",
    "    return np.array([i for i in range(instance.N+1) if i not in sequence])    \n",
    "\n",
    "@njit\n",
    "def transition_function_tsp(instance, k, sequence, neighbor):\n",
    "    return sequence + [neighbor]\n",
    "@njit\n",
    "def cost_function_tsp(instance, k, sequence, neighbor):\n",
    "    \n",
    "    if k < instance.N-1:\n",
    "        return instance.distance_matrix[sequence[k]][neighbor]\n",
    "    else:\n",
    "        return instance.distance_matrix[sequence[k]][neighbor] + instance.distance_matrix[neighbor][sequence[0]]\n",
    "\n",
    "dp_tsp = DP(feasible_decisions_tsp, transition_function_tsp,  cost_function_tsp, \"min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An instance reader function for the TSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsp_instance(filename):    \n",
    "    distance_matrix = tsplib_distance_matrix(filename)\n",
    "    return TSPInstance(distance_matrix, len(distance_matrix)-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsplib_file = \"./../problems/tsp/instances/brazil58.tsp\" # optimal solution 25395 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "#tsplib_file = \"./../problems/tsp/instances/berlin52.tsp\" # optimal solution  7542 (lt. http://comopt.ifi.uni-heidelberg.de/software/TSPLIB95/STSP.html)\n",
    "\n",
    "tsp_instance = read_tsp_instance(tsplib_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generic implementations of algorithms based on the generic DP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic greedy for DP models\n",
    "\n",
    "- given a generic DP model, we can now start devising generic implementation of algorithms operating on DP models\n",
    "- as an example, we can generically implement greedy as follows:\n",
    "\n",
    "**Observe:**\n",
    "- below, we avoid some loops by directly working on arrays of decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_greedy(dp, instance, state_start):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    solution_decisions = []\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(instance.N):    \n",
    "        \n",
    "        # get feasible decisions\n",
    "        decisions = dp.feasible_decisions(instance, k, state)\n",
    "        \n",
    "        if decisions is None or len(decisions) == 0: continue\n",
    "            \n",
    "        \n",
    "        # create a tuple (cost, decision) for each decision\n",
    "        costs_decisions =[(dp.cost_function(instance, k, state, d), d) for d in decisions]\n",
    "        \n",
    "        #get the best decision \n",
    "        best_cost, best_decision = best(costs_decisions, dp.direction) \n",
    "        \n",
    "        #get target state\n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += best_cost\n",
    "        solution_decisions.append(best_decision)\n",
    "        \n",
    "    return total_cost, solution_decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Try with both KP and TSP:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Knapsack Problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8817"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_greedy(dp_kp, kp_instance, 0)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "TSP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30774"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_greedy(dp_tsp, tsp_instance, [0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Greedy as a myopic policy \n",
    "\n",
    "- we will see later how to solve a $DP$ model to optimality\n",
    "\n",
    "- in general, we refer to a function $\\pi$ that maps a state $x_k$ to a decision $u_k$ as a **policy**\n",
    "- in a deterministic problem, given a policy $\\pi$, we can obtain a solution to $DP$ by \n",
    "  - starting from $x_k := x_0$ and selecting the $u_k$ according to the policy\n",
    "  - applying the state transition $x_{k+1} = f(x_k, u_k)$\n",
    "  - and continue until $k:= N -1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Greedy as a policy:\n",
    "- we can view the greedy algorithm as being based on a policy that selects a $u_k$ that minimizes the transition costs $g$:\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, g(x_k, u_k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observe: This policy is **myopic** since it does not account for how deciding for a certain $u_k$ affects the quality of the remaining solution process - hence its name: *greedy*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accounting for the future: The value function \n",
    "\n",
    "\n",
    "- to quantify the future value (also called cost-to-go) of a state $x_k$, we use the so-called value function $J(x_k)$ \n",
    "  - given a state $x_k$, $J(x_k)$ represents the **optimal** cost / value obtained by solving the residual problem from stages $k$ to $N-1$.\n",
    "  - the corresponding problem starting at $k$ is also referred to as the **tail subproblem**.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "   \n",
    "Given a value function, we can compute the decision to take in stage  $k$ as:\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, \\Big( g(x_k, u_k) + J(f(x_k, u_k)) \\Big) $$\n",
    "\n",
    "\n",
    "Observe that for the greedy policy for the knapsack, $J(x_{k}) = 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Q-values / Q-factors\n",
    "\n",
    "- in some cases, in particular in some reinforcement learning approaches, it is convenient to use so-called Q-factors,  Q-values or Q-functions\n",
    "\n",
    "$$Q_k(x_k, u_k) = g(x_k, u_k) + J\\big( f(x_k, u_k ) \\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "..using these Q-factors, we can re-write the problem of selecting the next decision / control / action as:\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, Q_k(x_k, u_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact and approximate value functions\n",
    "\n",
    "\n",
    "#### The exact (optimal) value function\n",
    "- we denote the exact / optimal value function with $J^*$.\n",
    "- if we have access to $J^*$, then the greedy policy based on $g(x_k, u_k) + J(f(x_k, u_k))$ gives us an optimal solution\n",
    "- the problem: $J^*$ is typically probitively hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Approximate value functions\n",
    "\n",
    "- we denote an approximate value function with $\\tilde{J}$\n",
    "- a greedy policy based on $\\tilde{J}$ is suboptimal,  but can be much faster to compute\n",
    "- approximate value functions can be determined in various ways $\\tilde{J}$:\n",
    "  - using offline training / learning\n",
    "  - using problem simplification or aggregation (solve an approximate tail problem)\n",
    "  - using online techniques (e.g. rollout), see later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact Dynamic Programming\n",
    "\n",
    "How to obtain an exact value function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In general,  this requires exactly solving the DP model\n",
    "- there are various ways to do this (backward or forward), we will consider this in somewhat more detail next week\n",
    "- for large-scale COP, exactly solving the DP model usually takes probhibitively long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/full_tree.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact Dynamic Programming: An Illustration of the Knapsack Case\n",
    "\n",
    "One approach to exactly solve a DP model is to\n",
    "- create the state transition graph and\n",
    "- compute the shortest (longest) path in the graph\n",
    "\n",
    "\n",
    "<img src=\"./img/reaching_05.png\" width=\"60%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exact Dynamic Programming by Reaching: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "    \n",
    "def add_to_visualization_graph(g, k, prev_state, state, decision, cost_decision, cost_state):\n",
    "\n",
    "    prev_node = (k,prev_state)\n",
    "    node = (k+1, state)\n",
    "    \n",
    "    g.add_edge(prev_node,node, label= \" u:\" + str(decision) + \"\\ng(x,u):\" + str(cost_decision) + \"\\n\\n\")\n",
    "    g.nodes()[node][\"xlabel\"]= cost_state\n",
    "    g.nodes()[prev_node][\"label\"] = prev_state\n",
    "    g.nodes()[node][\"label\"] = state\n",
    "\n",
    "def display_visualization_graph(g):\n",
    "    clear_output()\n",
    "    \n",
    "    ag = nx.nx_agraph.to_agraph(g)\n",
    "    ag.graph_attr[\"nodesep\"] = 0.5\n",
    "    svg = ag.draw(prog='dot',format='svg')\n",
    "    display(SVG(svg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def dp_by_reaching_visualized(dp, instance, start_state):\n",
    "    \n",
    "\n",
    "    ## for the visualization\n",
    "    g = nx.MultiDiGraph()\n",
    "    \n",
    "    \n",
    "    # we assume a stage-wise structure\n",
    "    states_costs = [{} for k in range(0,instance.N+1)]\n",
    "    \n",
    "    states_costs[0][start_state] = 0\n",
    "    states_costs[0][start_state] = 0\n",
    "    \n",
    "    for k in range(0,instance.N):\n",
    "        \n",
    "        for state, cost in states_costs[k].items():\n",
    "            \n",
    "            for decision in dp.feasible_decisions(instance, k, state):\n",
    "                \n",
    "                next_state = dp.transition_function(instance, k, state, decision)\n",
    "                next_cost = cost + dp.cost_function(instance, k, state, decision)\n",
    "            \n",
    "                if next_state not in states_costs[k+1]:\n",
    "                    states_costs[k+1][next_state] = next_cost\n",
    "\n",
    "                elif better(next_cost, states_costs[next_state], dp.direction):\n",
    "                    states_costs[k+1][next_state] = next_cost\n",
    "                \n",
    "                \n",
    "                add_to_visualization_graph(g, k, state, next_state, decision, dp.cost_function(instance, k, state, decision), next_cost)\n",
    "                \n",
    "                keyboard.read_key()\n",
    "                display_visualization_graph(g)\n",
    "                \n",
    "                    \n",
    "    return best (list(states_costs[instance.N].values()), dp.direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "values = [3000,2000,1500]\n",
    "weights = [30,20,15]\n",
    "capacity = 35\n",
    "kp_instance_tiny = KPInstance(values, weights, capacity, 3)\n",
    "\n",
    "\n",
    "#dp_by_reaching_visualized(dp_kp, kp_instance_tiny, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exact Dynamic Programming by Reaching: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dp_by_reaching(dp, instance, start_state):\n",
    "    \n",
    "    \n",
    "    # we assume a stage-wise structure\n",
    "    states_costs = [{} for k in range(0,instance.N+1)]\n",
    "    \n",
    "    states_costs[0][start_state] = 0\n",
    "    \n",
    "    for k in range(instance.N):\n",
    "        \n",
    "\n",
    "        \n",
    "        for state, cost in states_costs[k].items():\n",
    "                                               \n",
    "\n",
    "            for decision in dp.feasible_decisions(instance, k, state):                      \n",
    "                \n",
    "                next_state = dp.transition_function(instance, k, state, decision)\n",
    "                next_cost = cost + dp.cost_function(instance, k, state, decision)\n",
    "            \n",
    "                if next_state not in states_costs[k+1]:\n",
    "                    states_costs[k+1][next_state] = next_cost\n",
    "\n",
    "                elif better(next_cost, states_costs[k+1][next_state], dp.direction):\n",
    "                    states_costs[k+1][next_state] = next_cost\n",
    "                \n",
    "    return best (list(states_costs[instance.N].values()), dp.direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Question: How can we obtain the optimal solutoin (that is, the optimal sequence of decisions?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Does it work for the TSP as well?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tsp_instance_tiny = TSPInstance(distance_matrix_tiny, len(distance_matrix_tiny)-1)\n",
    "\n",
    "#dp_by_reaching(dp_tsp, tsp_instance_tiny, (0,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We have to make sure that the states are immutable types - in this case, we can use tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@njit\n",
    "def transition_function_tsp_tup(instance, k, sequence, neighbor):\n",
    "    return sequence + (neighbor,)\n",
    "\n",
    "dp_tsp_tup = DP(feasible_decisions_tsp, transition_function_tsp_tup,  cost_function_tsp, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_tsp_tup.transition_function(tsp_instance_tiny,0,(0,),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_by_reaching(dp_tsp_tup, tsp_instance_tiny, (0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DP by Reaching for the TSP with Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_by_reaching_visualized(dp_tsp_tup, tsp_instance_tiny, (0,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exact Dynamic Programming by Reaching is very similar to Breadth-First Search\n",
    "\n",
    "- in fact, we can see DP by reaching as a variant of BFS with memoization\n",
    "  - that is, the same state is never expanded twice in each layer / stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Heuristic Search Algorithms based on DP Formulations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Motivation\n",
    "\n",
    "- given the fact that many CO problems are NP hard, exact DP suffers from the so-called curse of dimensionality\n",
    "- on the other hand, greedy is very simplistic and may yield low-quality solutions\n",
    "- one idea to improve upon greedy is to expand more that one state per layer / stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beam Search\n",
    "- in each stage (each layer) only expand a limited number (`beam_width`) of nodes / states \n",
    "- `beam_width=1` is a greedy algorithm\n",
    "- `beam_width=â` is the exact BFS / DP by Reaching algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def best(collection, direction):\n",
    "    if direction == \"min\":\n",
    "        return min(collection)\n",
    "    else:\n",
    "        return max(collection)\n",
    "     \n",
    "def nbest(n,collection, direction):\n",
    "    if direction == \"min\":\n",
    "        return nsmallest(n,collection)\n",
    "    else:\n",
    "        return nlargest(n,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def beam_search_visualized(dp, instance, start_state, beam_width):\n",
    "       \n",
    "\n",
    "    ## for the visualization\n",
    "    g = nx.MultiDiGraph()\n",
    "    \n",
    "    cost_state_tuples = [(0, start_state)]\n",
    "\n",
    "\n",
    "    for k in range(0,instance.N):\n",
    "        \n",
    "                   \n",
    "        nbest_cost_state_tuples = nbest(beam_width, cost_state_tuples, dp.direction)\n",
    "    \n",
    "        cost_state_tuples = []\n",
    "        \n",
    "        for cost, state in nbest_cost_state_tuples:           \n",
    "            \n",
    "            for decision in dp.feasible_decisions(instance, k, state):\n",
    "                \n",
    "                next_state = dp.transition_function(instance, k, state, decision)\n",
    "                next_cost = cost + dp.cost_function(instance, k, state, decision)\n",
    "           \n",
    "                cost_state_tuples.append((next_cost, next_state))\n",
    "\n",
    "                \n",
    "    return #best(cost_state_tuples, dp.direction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_width = 2\n",
    "beam_search_visualized(dp_tsp, tsp_instance_tiny, [0], beam_width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Beam Search: Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def best(collection, direction):\n",
    "    if direction == \"min\":\n",
    "        return min(collection)\n",
    "    else:\n",
    "        return max(collection)\n",
    "@njit   \n",
    "def nbest(n,collection, direction):\n",
    "    if direction == \"min\":\n",
    "        return nsmallest(n,collection)\n",
    "    else:\n",
    "        return nlargest(n,collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def beam_search(dp, instance, start_state, beam_width):\n",
    "        \n",
    "    # we assume a stage-wise structure\n",
    "    \n",
    "    cost_state_tuples = [(0, start_state)]\n",
    "\n",
    "\n",
    "    for k in range(0,instance.N):\n",
    "        \n",
    "        #cost_state_tuples = [(cost, state) for state, cost in states_costs[k].items()]\n",
    "                   \n",
    "        nbest_cost_state_tuples = nbest(beam_width, cost_state_tuples, dp.direction)\n",
    "    \n",
    "        cost_state_tuples = []\n",
    "        \n",
    "        for cost, state in nbest_cost_state_tuples:           \n",
    "            \n",
    "            for decision in dp.feasible_decisions(instance, k, state):\n",
    "                \n",
    "                next_state = dp.transition_function(instance, k, state, decision)\n",
    "                next_cost = cost + dp.cost_function(instance, k, state, decision)\n",
    "          #  \n",
    "                cost_state_tuples.append((next_cost, next_state))\n",
    "\n",
    "                \n",
    "    return best(cost_state_tuples, dp.direction)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(dp_kp, kp_instance_tiny, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30774"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beam_search(dp_tsp, tsp_instance, [0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Approximate value functions / approximation in value space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exact and approximate value functions\n",
    "\n",
    "\n",
    "#### The exact (optimal) value function\n",
    "- we denote the exact / optimal value function with $J^*$.\n",
    "- if we have access to $J^*$, then the greedy policy based on $g(x_k, u_k) + J(f(x_k, u_k))$ gives us an optimal solution\n",
    "- the problem: $J^*$ is typically probitively hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Approximate value functions\n",
    "\n",
    "- we denote an approximate value function with $\\tilde{J}$\n",
    "- a greedy policy based on $\\tilde{J}$ is suboptimal,  but can be much faster to compute\n",
    "- approximate value functions can be determined in various ways $\\tilde{J}$:\n",
    "  - using offline training / learning\n",
    "  - using problem simplification or aggregation (solve an approximate tail problem)\n",
    "  - using online techniques (e.g. rollout), see later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we obtain an approximate value function?\n",
    "\n",
    "#### Offline value function approximation\n",
    "\n",
    "- training machine learning models using given solutions obtained from\n",
    "  - exact or heuristic solution approaches\n",
    "  - of \"self-generated solutions\" obtained using reinforcement learning methods\n",
    "\n",
    "\n",
    "#### Online value function approximation\n",
    "\n",
    "- instead of training a model, one obtains $\\tilde{J}$ by heuristically solving the tail subproblem using\n",
    "  - so-called rollout with a base heuristic\n",
    "  - possibly combined with so-called multi-step lookahead\n",
    "- this is what we will discuss now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Online approximation in value space\n",
    "\n",
    "We will consider the following approaches now:\n",
    "- rollout with greedy as base heuristic\n",
    "- simplified rollout\n",
    "- (simplified) multi-step lookahead with rollout \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimal and approximate value functions\n",
    "\n",
    "\n",
    "#### The optimal (exact) value function\n",
    "- we denote the exact / optimal value function with $J^*$.\n",
    "- if we have access to $J^*$, then the greedy policy based on $g(x_k, u_k) + J(f(x_k, u_k))$ gives us an optimal solution\n",
    "- the problem: $J^*$ is typically probitively hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Approximate value functions\n",
    "\n",
    "- we denote an approximate value function with $\\tilde{J}$\n",
    "- a greedy policy based on $\\tilde{J}$ is suboptimal,  but can be much faster to compute\n",
    "- approximate value functions can be determined in various ways $\\tilde{J}$:\n",
    "  - using offline training / learning\n",
    "  - using problem simplification or aggregation (solve an approximate tail problem)\n",
    "  - using online techniques (e.g. rollout), see later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic Rollout with a Base Heuristic\n",
    "\n",
    "- recall that given an approximate value function $\\tilde{J}$, we can construct a policy that takes the decision according to the best approximate $Q-$-value $\\tilde{Q}_k(x_k, u_k)$\n",
    "\n",
    "$$u_k = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, \\tilde{Q}_k(x_k, u_k) = \\underset{u_k \\in U_k(x_k)}{\\operatorname{argmin}} \\, \\Big( g(x_k, u_k) +  \\tilde{J}(f(x_k, u_k)) \\Big) $$\n",
    "  \n",
    "  \n",
    "- key idea of rollout:  run a (simple and fast) base heuristic on the tail subproblem starting from $x_{k+1} = f(x_k, u_k)$ to obtain a cost / value $H(f(x_k, u_k))$, and use that value as value function approximation:\n",
    "  - $\\tilde{J}(x_{k+1}) = H (x_{k+1})$\n",
    "\n",
    "\n",
    "<img src=\"./img/rollout_general.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preparing the Greedy base heuristic\n",
    "\n",
    "- we modify the greedy algorithm such that it can start in any stage \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def dp_greedy_from_stage(dp, instance, state_start, from_k):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    solution_decisions = []\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(from_k, instance.N):    \n",
    "        \n",
    "        # get feasible decisions\n",
    "        decisions = dp.feasible_decisions(instance, k, state)\n",
    "        \n",
    "        if decisions is None: continue\n",
    "        \n",
    "        # create a tuple (cost, decision) for each decision\n",
    "        costs_decisions =[(dp.cost_function(instance, k, state, d), d) for d in decisions]\n",
    "        \n",
    "        #get the best decision \n",
    "        best_cost, best_decision = best(costs_decisions, dp.direction) \n",
    "\n",
    "\n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += best_cost\n",
    "        solution_decisions.append(best_decision)\n",
    "        \n",
    "    return solution_decisions, total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generic rollout with greedy base heuristic in Python\n",
    "- given that we have a generic greedy, we can also implement a generic rollout algorithm\n",
    "- we first write a function that gives us a q-value using rollout for approximating the cost-to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_q_value_rollout(dp, instance, k, state, decision):\n",
    "    \n",
    "    # one-step cost (function g):\n",
    "    cost = dp.cost_function(instance,k, state, decision)\n",
    "    \n",
    "    # compute next state\n",
    "    next_state = dp.transition_function(instance, k, state, decision)\n",
    "    \n",
    "    # compute cost to go via rollout\n",
    "    cost_to_go = dp_greedy_from_stage(dp, instance, next_state, k+1)[1]\n",
    "    \n",
    "    return cost + cost_to_go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7871"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_q_value_rollout(dp_kp, kp_instance, 2, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Generic rollout with greedy base heuristic in Python\n",
    "- now the main function\n",
    "- observe: by using `@njit(parallel=True)`, numba will try parallelzing the list comprehension involving the dp_greedy call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit(parallel=True)\n",
    "def dp_rollout(dp, instance, state_start):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(0, instance.N):    \n",
    "         \n",
    "        decisions = dp.feasible_decisions(instance, k, state)    \n",
    "        \n",
    "        if decisions is None: continue\n",
    "        \n",
    "        # create a tuple (cost, decision) for each decision\n",
    "        q_values_decisions = [ (get_q_value_rollout(dp, instance, k, state, d), d) for d in decisions]\n",
    "       \n",
    "\n",
    "        #get the best decision \n",
    "        best_q_value, best_decision = best(q_values_decisions, dp.direction) \n",
    "        \n",
    "        total_cost += dp.cost_function(instance, k, state, best_decision)\n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "        \n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8929"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_rollout(dp_kp, kp_instance,0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 141 ms\n",
      "Wall time: 138 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28131"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_rollout(dp_tsp, tsp_instance,[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Rollout: Some comments\n",
    "\n",
    "\n",
    "- applying rollout is always at least as good as running only the base heuristic\n",
    "  - (as long as certain very natural conditions are satisified)\n",
    "- the base heuristic is not restricted to greedy heuristics, but may also involve\n",
    "  - ML-based policies, e.g. policies resulting from applying a \"policy (neural) network\"\n",
    "  - multiple different base heuristics\n",
    "- given that our policy involves one \"exact\" step before running the rollout, we call this aproach one-step lookahead minimization with rollout\n",
    "- one-step lookahead minimization also works with other value function approximations (e.g. those obtained from offline training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximate one-step minimization: Simplified rollout\n",
    "\n",
    "\n",
    "- applying to every decision may take a lot of time\n",
    "- this will be even more true for multi-step lookahead\n",
    "\n",
    "\n",
    "- in order to speed up the solution process, we can approximate the minimization step by not considering every single $u_k \\in U_k(x_k)$ but only the \"most promising\"\n",
    "  - as an example, we can use the greedy criterion $g(x_k,u_k)$ as criterion for restricting the decisions to consider.\n",
    "  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def dp_simplified_rollout(dp, instance, state_start, n_candidates):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(0, instance.N):    \n",
    "         \n",
    "        decisions = dp.feasible_decisions(instance, k, state) \n",
    "        \n",
    "        if decisions is None: continue        \n",
    "        \n",
    "        costs_decisions = [(dp.cost_function(instance, k, state, d), d) for d in decisions]\n",
    "      \n",
    "        ## take only the best decisions!!\n",
    "        best_costs_decisions = nbest(n_candidates, costs_decisions, dp.direction)        \n",
    " \n",
    "        # create a tuple (cost, decision) for each decision\n",
    "        q_values_decisions = [ (get_q_value_rollout(dp, instance, k, state, d), d) for (c,d) in best_costs_decisions]\n",
    "       \n",
    "\n",
    "        #get the best decision \n",
    "        best_q_value, best_decision = best(q_values_decisions, dp.direction) \n",
    "        \n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += dp.cost_function(instance, k, state, best_decision)\n",
    "        \n",
    "    return total_cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 750 ms\n",
      "Wall time: 749 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8929"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_simplified_rollout(dp_kp, kp_instance, 0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 906 ms\n",
      "Wall time: 908 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28151"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_simplified_rollout(dp_tsp, tsp_instance,[0],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Informed Beam Search with Rollout\n",
    "\n",
    "- we also use rollout in combination with Beam Search (How?)\n",
    "- this type of search called informed search, and rollout can be used as a \"heuristic function\" (in the sense of tree search)\n",
    "- note that a very famous variant of informed search is A* search where the \"heuristic function\" provides a relaxation (and thus is an admissible heuristic\n",
    "\n",
    "**Excercise:** Implement an informed Beam Search using Rollout as heuristic function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-step lookahead\n",
    "\n",
    "- in (exact) dynamic programming (by reaching), we (somewhat) construct a full state-transition graph\n",
    "- in rollout, in each iteration, only the first step is \"exact\", the rest of the graph is approximated\n",
    "- in multi-step lookahead, we partially expand the tree for (more than one stage) to have more \"exact\" steps before using a value function approximation for selection\n",
    "- below: multi-step lookahead with rollout for value function approximation\n",
    "\n",
    "<img src=\"./img/multistep_lookahead.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Very important:\n",
    "- at each step $k$, only a single $u_k$ is selected - all the computations of the $u_{k+1}$ are only performed to get a better $\\tilde{J}$!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplified Multi-step lookahead: An Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def dp_simplified_multi_stage_lookahead_rollout(dp, instance, k_start, state_start, n_candidates, n_lookahead_steps = 1):\n",
    "    \n",
    "    state = state_start\n",
    "    \n",
    "    total_cost = 0\n",
    "    \n",
    "    for k in range(k_start, instance.N):    \n",
    "         \n",
    "        decisions = dp.feasible_decisions(instance, k, state) \n",
    "        \n",
    "        if decisions is None: continue\n",
    "        \n",
    "        \n",
    "        costs_decisions = [(dp.cost_function(instance, k, state, d), d) for d in decisions]\n",
    "        \n",
    "        best_costs_decisions = nbest(n_candidates, costs_decisions, dp.direction)\n",
    "        \n",
    " \n",
    "        # create a tuple (cost, decision) for each decision\n",
    "    \n",
    "        q_values_decisions = [ (get_q_value_multi_step_lookahead_rollout(dp, instance, k, state, d, n_candidates, n_lookahead_steps), d) for (c,d) in best_costs_decisions]\n",
    "       \n",
    "\n",
    "\n",
    "        #get the best decision \n",
    "        best_q_value, best_decision = best(q_values_decisions, dp.direction) \n",
    "        \n",
    "        state = dp.transition_function(instance, k, state, best_decision)\n",
    "\n",
    "        total_cost += dp.cost_function(instance, k, state, best_decision)\n",
    "        \n",
    "    return state, total_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplified Multi-step lookahead: An Implementation (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_q_value_multi_step_lookahead_rollout(dp, instance, k, state, decision, n_candidates, n_lookahead_steps):\n",
    "    \n",
    "    # one-step cost (function g):\n",
    "    cost = dp.cost_function(instance,k, state, decision)\n",
    "    \n",
    "    if n_lookahead_steps == 0:\n",
    "        return cost\n",
    "    \n",
    "    # compute next state\n",
    "    next_state = dp.transition_function(instance, k, state, decision)\n",
    "    \n",
    "    if n_lookahead_steps == 1:\n",
    "    # compute cost to go via rollout\n",
    "        cost_to_go = dp_greedy_from_stage(dp, instance, next_state, k+1)[1]\n",
    "    else:\n",
    "        cost_to_go = dp_simplified_multi_stage_lookahead_rollout(dp, instance, k+1, next_state, n_candidates, n_lookahead_steps - 1)[1]\n",
    "    \n",
    "    return cost + cost_to_go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26710"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp_simplified_multi_stage_lookahead_rollout(dp_tsp, tsp_instance, 0, [0], 6, 2)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrapping up and Outlook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximation in policy space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- so far, we discussed approximation in value space (approximating the value function $J$)\n",
    "- we then always used a policy by doing one-step lookahead minimization using the approximate value function $\\tilde{J}$:\n",
    " \n",
    " $$u_k = \\underset{u_k \\in U_k}{\\operatorname{argmin}} \\, g(x_k, u_k) + \\tilde{J}(x_{k+1})$$\n",
    "\n",
    "- however, it is also possible approximate policies $\\pi$ that directly give us a decision (without that minimization):\n",
    "\n",
    "$$u_k = \\pi(x_k)$$\n",
    "\n",
    "As an example, such an approximate $\\pi$ can be obtained via offline training:\n",
    "- optimizing a parameterized policy function (e.g. a linear decision rule)\n",
    "- in case of discrete decisions: training / learning a classification model based on given policies or by reinforcement learning (self-learning)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Observe:** \n",
    "\n",
    "- a (learned) approximate policy can be used as a base policy for a rollout algorithm\n",
    "- an offline learned policy can often be substantially improved by including some lookahead and rollout steps (\"online play\" in games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simplified AlphaZero architecture as an example for a hybrid approach\n",
    "\n",
    "<img src=\"./img/alpha_zero_sketch.png\" width=\"60%\">\n",
    "\n",
    "- \"Position evaluator\" is a value function approximation\n",
    "- rollout is not fully performed but truncated; at the end of the rollout, an approximate value function is used to account for the future value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References / Going deeper\n",
    "\n",
    "<img src=\"./img/lessons_az.jpg\" width=\"20%\" align=\"right\">\n",
    "\n",
    "- much of the notation and most figures are taken from presentations and books from D. Bertsekas\n",
    "- Bertsekas has many books, his most recent one (see on the right) is available for free\n",
    "- he also has a couple of lectures and courses available online\n",
    "- you can find links to all his materials on his website https://www.mit.edu/~dimitrib/home.html\n",
    "  - in particular in the section http://web.mit.edu/dimitrib/www/RLbook.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions and Outlook\n",
    "\n",
    "\n",
    "#### This week, we...\n",
    "- got used to the concept of a DP model and dynamic programming\n",
    "- learned how to use simple greedy heuristics to derive relatively powerful heuristic solutions approaches\n",
    "- maybe got a first or different perspective on the relation of DP and reinforcement learning\n",
    "\n",
    "#### Next week, we...\n",
    "- will have a closer look at exact approaches for solving DP models\n",
    "- will discuss so-called Decision Diagrams which\n",
    "  - provide an exact tecnnique for reducing the state-transition graph of a DP model\n",
    "  - provide a generic mechanism for obtaining combinatorial relaxations from DP models\n",
    "  - and, building on that, allow constructing an interesting Branch-and-Bound scheme that does not rely on LP relaxations"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:dpmodels2024]",
   "language": "python",
   "name": "conda-env-dpmodels2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
